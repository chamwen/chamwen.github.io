---
layout: post
title: 机器学习中的分布差异度量方法
date: 2020-04-02 9:35
tags: [meausre, entropy]
categories: Machine-Learning
description: "分布差异主要描述两个分布（一般是二维矩阵）之间的差异，机器学习中常用的分布差异度量方法包括：基于熵的信息熵、互信息、条件熵、交叉熵、KL 散度、JS 散度以及 Wasserstein 距离等，其含义、理论总结"
mathjax: true
---

* content
{:toc}
分布差异主要描述两个分布（一般是二维矩阵）之间的差异，机器学习中常用的分布差异度量方法包括：基于熵的信息熵、互信息、条件熵、交叉熵、KL 散度、JS 散度以及 Wasserstein 距离等，其含义、理论总结。 <!--more-->

**Cham's Blog 首发原创**



### 1. 信息熵 (Entropy)

熵 Entropy 是一种量化数据中的信息的单位，一般用 $H$ 表示。分布的熵的公式如下：

$$
H(p)=-\sum_{i=1}^Np(x_i)·\text{log}\ p(x_i)
$$

当对数底为 2 时，表示的是编码概率分布 $p$ 所需要的最少二进制位个数。



### 2. 互信息 (Mutual Information)

无监督学习中常用的损失函数，作用于标签时，最大化预测标签和真实标签的信息熵，可以促使预测标签 certain 且 diverse，

$$
\begin{align}
I(X;Y)&=\sum_{x,y}p(x,y)·\text{log}\ \frac {p(x, y)} {p(x),p(y)}\\
&=-\sum_y p(y)\log p(y) - \sum_xp(x)H(Y|X=x)\\
&=H(Y)-H(Y|X)
\end{align}
$$

直观地说，如果把熵 $H(Y)$ 看作一个随机变量于不确定度的量度，那么 **$H(Y\|X)$ 就是 在已知 $X$ 事件后 $Y$ 事件会发生** 的不确定度。互信息为 $Y$ 的熵减去条件熵（见4）。



### 3. KL散度 (KullbacK-Leibler Divergence)

也称相对熵。熵的大小可以度量编码 $p$ 最少需要多少空间，而 KL 散度则是**衡量使用一个概率分布代表另一个概率分布所损失的信息量**。

$$
\begin{align}
D_{KL}(p||q)&=\sum_{i=1}^Np(x_i)·\left(\text{log}\ (p(x_i)-\text{log}\ (q(x_i)\right)\\
&=\sum_{i=1}^Np(x_i)·\text{log}\frac {p(x_i)} {q(x_i)}\\
&=\sum_{i=1}^Np(x_i)·\text{log}\ p(x_i)-\sum_{i=1}^Np(x_i)·\text{log}\ q(x_i)\\
&=H(p||q)-H(p)
\end{align}
$$

$p$ 为真实分布，使用 $q$ 来近似 $p$。
由公式可以看出，$D_{KL}(p||q)$ 就是 $q$ 和 $p$ 对数差值关于 $p$ 的期望，所以 KL 散度表示如下:
$$
D_{KL}(p||q)=E[\text{log}\ p(x)-\text{log}\ q(x)]
$$

**注意：**
1）如果继续用 2 为底的对数计算，则 KL 散度值表示信息损失的二进制位数。
2）如果 $p$ 和 $q$ 是同分布的，则 KL 散度为 0。
3）KL 散度不是距离，因为不符合对称性，所以用 KL 散度度量分布差异时需设计成对称的，$D_{KL}(p||q)+D_{KL}(q||p)$

Specializing to Gaussian measures $P\sim\mathcal{N}(\mu_1, \Sigma_1)$ and $Q\sim\mathcal{N}(\mu_2, \Sigma_2)$, then the KL divergence is given by

$$
\text{KL}(P||Q)=\frac 1 2 [(\mu_2-\mu_1)^{\top}\Sigma_2^{-1}(\mu_2-\mu_1)+\text{trace}(\Sigma_2^{-1}\Sigma_1)-\text{ln}(\frac {det(\Sigma_1)} {det(\Sigma_2)})-K]
$$



### 4. 条件熵 (Conditional Entropy)

条件熵是在已知随机变量 X 的条件下，Y 的条件概率分布的熵对随机变量 X 的数学期望

$$
\begin{align}
H(Y|X) &=\sum_{x\in\mathcal{X}} p(x) H(Y|X=x) \\ 
&=-\sum_{x\in\mathcal{X}} p(x) \sum_{y\in\mathcal{Y}} p(y|x) \log p(y|x)\\
&=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}} p(x,y)\log p(y|x)\\
&=-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log p(y|x)\\
\end{align}
$$

最小化条件熵让模型远离决策边界，可以应用在无监督数据上，以利用其数据分布信息。



### 5. 交叉熵 (Cross entropy)

1) **Cross entropy** 

$$
\begin{align}
H(p||q)&=-\sum_{i=1}^Np(x_i)·\text{log}\  {q(x_i)}\\
&=D_{KL}(p||q)+H(p)
\end{align}
$$

为什么深度学习中用交叉熵而不用 KL 散度？
$H(p||q)$ 中 $p$ 代表数据的真实分布，数据已经给定；$q$ 代表模型学到的分布，真实数据的熵 $H(p)$ 是固定的，对于最小化问题等价。

2) **和 softmax 结合应用在深度学习中**
softmax 原理
$$
\sigma ( \mathbf { z } ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } } \quad \text { for } j = 1 , \ldots , K
$$

其中 $z_j$ 为神经元输出。然后基于交叉熵和 softmax 归一化的 loss

$$
L=-\frac 1 N\sum_{i=1}^Ny_i\ \text{log}\ \frac {e^{f(x_i)}} {\sum e^{f(x_i)}}
$$



###  6. JS 散度(Jensen-Shannon)

JS 散度度量了两个概率分布的相似度，基于 KL 散度的变体，解决了 KL 散度非对称的问题。一般地，JS 散度是对称的，其取值是 0 到 1 之间。定义如下：

$$
JS(p||q)=\frac 1 2 KL(p||\frac {p+q} 2)+\frac 1 2 KL(q||\frac {p+q} 2)
$$

KL 散度和 JS 散度度量的时候有一个问题：
如果两个分布 p, q 离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0，梯度消失了。



### 7. Wasserstein 距离

Wasserstein 距离度量两个概率分布之间的距离，定义如下：

$$
W(P_1,P_2)=\inf_{\gamma \sim \prod (P_1,P_2)} E_{(x,y)\sim\gamma}[||x-y||]
$$

$\prod (P_1,P_2)$ 是 $P1$ 和 $P2$ 分布组合起来的所有可能的联合分布的集合。对于每一个可能的联合分布 $\gamma$，可以从中采样 $(x,y)\sim\gamma$ 得到一个样本 $x$ 和 $y$，并计算出这对样本的距离 $\|x−y\|$，所以可以计算该联合分布 $\gamma$ 下，样本对距离的期望值 $E_{(x,y)∼\gamma} [\|x−y\|]$。在所有可能的联合分布中能够对这个期望值取到的下界就是 Wasserstein 距离。

直观上可以把 $E_{(x,y)∼\gamma} [\|x−y\|]$ 理解为在 $\gamma$ 这个路径规划下把土堆 P1 挪到土堆 P2 所需要的消耗。而 Wasserstein 距离就是在最优路径规划下的最小消耗。所以 Wasserstein 距离又叫 Earth-Mover 距离。

Wasserstein 距离相比 KL 散度和 JS 散度的**优势**在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能反映两个分布的远近。而 JS 散度在此情况下是常量，KL 散度可能无意义。

Specializing to Gaussian measures $P\sim\mathcal{N}(\mu_1, \Sigma_1)$ and $Q\sim\mathcal{N}(\mu_2, \Sigma_2)$, then the $2$-Wasserstein metric is given by

$$
W_2^2(P,Q)=\|\mu_1-\mu_2\|_2^2+\text{tr}(\Sigma_1+\Sigma_2-2(\Sigma_1^{\frac 1 2}\Sigma_2\Sigma_1^{\frac 1 2})^{\frac 1 2})
$$

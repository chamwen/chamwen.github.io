---
layout: post
title: 贝叶斯估计学习笔记
date: 2018-12-1 11:01
tags: Bayes
categories: Math
description: "华中科技大学贝叶斯估计课程笔记，知识点总结，分布，先验"
mathjax: true
---

* content
{:toc}
华中科技大学统计学院贝叶斯估计课程笔记，知识点总结，有关贝叶斯统计思想，常用分布、公式以及贝叶斯推断相关知识。<!--more-->

**Cham's Blog 首发原创**



### 贝叶斯统计思想

- 任意一个参数都是未知变量，都可以用一个概率分布去描述
- 贝叶斯统计的四大信息，总体、样本、先验信息和损失函数
- 样本信息可以修正参数的先验信息，得到更合理的参数分布，突出了先验信息  <!--more-->



### 贝叶斯统计基础

##### 1. 符号约定

贝叶斯领域 $\vec x$ 表示样本，$x$ 或者 $X$ 表示总体。在未说明单样本的情况下 $\vec x=\{x_1,\dots x_n\}$，一般的描述，设随机变量 $X$ 服从某分布（总体），或者设$x_1,\dots x_n$ 是来自某分布的一个样本。

##### 2. 总体、样本和联合分布信息

总体信息反应的是数据整体的内在规律，样本信息是通过从总体中进行抽样得到的。总体分布不是联合分布，总体分布和样本分布都是用似然函数来描述，似然函数和参数先验分布之积才是联合分布，联合分布综合考虑了样本信息、参数先验信息和总体信息。

- 总体分布 $p(x\mid \theta)$ 
- 样本分布 $p(\vec x\mid \theta)=\prod_{i=1}^np(x_i\mid \theta)$
- 联合分布 $h(\vec x,\theta)=p(\vec x\mid \theta)\pi(\theta)$

##### 3. 伯努利分布

两点分布，离散型概率分布，随机变量只有两种取值

$$
f_{X}(x)=p^{x}(1-p)^{1-x}=\left\{ {\begin{matrix}p&{\mbox{if } }x=1,\\q\ &{\mbox{if } }x=0.\\\end{matrix} }\right.
$$

##### 4. 二项分布

用于预测潜在事件发生次数时使用二项分布，$X\sim B(n,p)$ ，$p(X=\vec x\mid \theta)=C_n^xp^x(1-p)^{n-x},\ x=0,\dots,n$ 表示在参数 $\theta$ 下抽样得到样本 $\vec x$ 的概率，由于有放回采样，成功概率 $p$ 表示的是总体信息中潜在事件发生的概率

##### 5. 贝塔分布

$X\sim Be(\alpha,\beta),\alpha>0,\beta>0$

$p(x)=\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},\ 0 \le x\le 1$

$Be(1,1)=U(0,1)$，贝塔分布和二项分布的核相同

$EX=\frac \alpha {\alpha+\beta},\ VarX=\frac {\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$

二项分布 $Be(n,\theta)$ 中的成功概率 $\theta$ 若取 $Be(1,1)$，则其后验分布为 $Be(x+1,n-x+1)$

##### 6. 伽玛分布

伽玛函数性质，$\Gamma(\alpha+1)=\alpha\Gamma(\alpha)$，$\Gamma(1)=1$

$X\sim Ga(\alpha,\lambda),\alpha>0,\lambda>0，其中\lambda为尺度参数$

$p(x)=\frac {\lambda^{\alpha} }{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x},\ x>0$

$EX=\frac \alpha \lambda,\ VarX=\frac \alpha {\lambda^2}$

##### 7. 倒伽玛分布

$X\sim IGa(\alpha,\lambda),\alpha>0,\lambda>0$

$p(x)=\frac {\lambda^{\alpha} }{\Gamma(\alpha)}x^{-(\alpha+1)}e^{-\lambda/ x},\ x>0$

$EX=\frac \lambda {\alpha-1},\ \alpha>1$

$ VarX=\frac {\lambda^2} {(\alpha-1)^2(\alpha-2)}$

##### 8. 泊松分布

离散概率分布，用于描述单位时间内随机事件**发生的次数**的概率分布，$X\sim P(\lambda)$

$p(\vec x=k) = \frac{\lambda^k}{k!}e^{-\lambda},\ \lambda>0$

参数λ是单位时间（或单位面积）内随机事件的平均发生率

##### 9. 指数分布

连续概率分布，用来表示独立随机事件**发生的时间间隔**，$X\sim Exp(\lambda)$

$p(x)=\lambda e^{-\lambda x},\ X>0$

$\lambda$ 的后验分布为 $Ga(n+\alpha, \beta+n\bar x)$，其中 $n$ 为样本容量

##### 10. 正态分布

正态分布的 $3\sigma$ 原则，可以用来检验用正态分布拟合数据分布的可行性

累积分布函数 $\Phi(z)=P(X\le z)$，正态分布的 $\alpha$ 分位数指的是累积概率为 $\alpha$ 的 $z$ 点



### 贝叶斯常用公式

##### 1. 序贯

$$
\begin{gather}
&\pi(\theta\mid x_1,x_2)\varpropto p(x_2\mid \theta)p(\theta\mid x_1)\\
&当x_1,\dots,x_n先后发生时，\pi(\theta\mid \vec x)\varpropto p(x_n\mid \theta)p(\theta\mid x_1,\dots,x_{n-1})\\
\end{gather}
$$

##### 2. 共轭先验分布

正态均值（方差已知）的共轭先验分布为 $N(\mu,\sigma^2)$
正态方差（均值已知）的共轭先验分布为 $IGa(\alpha,\lambda)$
多参数模型 $\pi(\mu,\sigma^2)$，均值和方差联合 $N-IGa$ 分布的共轭先验分布还是 $N-IGa$
二项分布的成功概率的共轭先验分布是 $Be(\alpha,\beta)$
泊松分布的均值的共轭先验分布是  $Ga(\alpha,\lambda)$
指数分布的均值倒数的共轭先验分布是  $Ga(\alpha,\lambda)$

##### 3. 正态均值（方差已知）的共轭先验分布

设  $x_1,\dots,x_n$ 是来自 $N(\theta,\sigma^2)$，其中 $\theta$ 的先验分布为 $N(\mu,\tau^2)$，则 $\theta$ 的后验分布的均值和方差分别为 $\mu_1$ 和 $\tau_1^2$

$$
\mu_1=\frac {\bar x\sigma_0^{-2}+\mu\tau^{-2} }{\sigma_0^{-2}+\tau^{-2} },\quad \frac 1 {\tau_1^2}=\frac 1 {\sigma_0^2}+\frac 1 {\tau^2},\quad \sigma_0^2=\sigma^2/n,\quad \bar x=\frac 1 n\Sigma_1^nx_i
$$

可以看出后验分布算出的均值是先验均值和样本均值的加权，当样本量大时取决于样本均值

##### 4. 边缘分布和先验分布

领域的边缘概率分布体现无标数据的聚类结构，领域的条件概率分布体现标注数据的判别结构，先验分布是主观概率的体现。边缘分布是一种混合分布，是由有限个密度函数混合而成，也就是不同类别数据额混合在一起。

$$
m(x)=\left\{ {\begin{matrix}\int_\Theta p(x\mid \theta)\pi(\theta)d\theta, &{当\ \theta\  连续时.}\\\sum_{\theta\in\Theta}p(x\mid \theta)\pi(
\theta),\ &{当\ \theta\ 离散时.}\\\end{matrix} }\right.
$$

##### 5. 收益函数、损失函数

收益函数，$Q(\theta,a)$ 对应状态集和行动集

损失函数，用 $L(\theta,a)$ 表示，表示在状态 $\theta$ 下采取行动 $a$ 对应的损失和改状态下最优行动相比的损失

##### 6. 高斯函数的定积分

$$
{\displaystyle \int _{-\infty }^{\infty }e^{-a(x+b)^{2}}\,dx={\sqrt {\frac {\pi }{a}}}.}
$$



### EM算法、极大似然估计MLE、后验概率最大化MAP和贝叶斯估计对比

##### 1. EM和MLE区别

极大似然估计MLE用于估计已知分布中的某个未知参数。收集数据后，通过写出对数似然函数并求其极大值点来获得参数的估计。EM算法也是估计已知分布中的某个未知参数，但不同的是分布可能是多元的 $p(x,z)$，其中 $X$ 是能够收集到的变量，而 $Z$ 不能（latent variable）。

E步是指expectation，计算的是对数似然函数在 $X$ 给定 $Z$ 这个条件分布下的期望。因为对数似然函数依赖于 $Z$，所以不能直接求极值。E步就相当于在求某个局部对数似然函数。M指Maximize。就是对上一步求出的期望求极大似然估计。求出的极大似然估计再代入上一步求条件分布，如此迭代直到收敛。

其实就是因为不能观测 $Z$，就必须把各种不同的 $\theta$ 和数据 $X$ 的组合用来求各种不同的 $Z$，然后再在各种不同的 $Z$ 的分布下求极大似然估计。每次E步算出的对数似然函数的期望都是实际对数似然函数的一个下界（Jensen不等式），通过不断更新这个下界，最终会找到极大似然估计。

更多参考，[机器学习系列之EM算法](https://www.cnblogs.com/Gabby/p/5344658.html)

##### 2.后验概率最大化MAP和MLE

最大似然估计是求参数 $\theta$, 使似然函数 $P(x_0\mid \theta)$ 最大。最大后验概率估计则是想求 $θ$ 使 $P(x_0\mid \theta)P(\theta)$ 最大。求得的 $θ$ 不仅让似然函数大，$\theta$ 自己出现的先验概率也得大（这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而 MAP 里是利用乘法）。MLE 是先验为均匀分布的特殊情况。

##### 3.贝叶斯估计和MLE

最大后验估计、后验中位数估计、后验期望估计都称为贝叶斯估计，贝叶斯估计选择了具有三种信息的后验分布。最大似然估计认为 $\theta$ 是个确定的矢量；贝叶斯估计认为 $\theta$ 是个随机变量 ， 以一定的概率分布取所有可能的值。

##### 4.总结

最小二乘的解析解可以用 Gaussian 分布和极大似然估计求得<br>
Ridge 回归可以用 Gaussian分布和最大后验估计解释<br>
LASSO 回归可以用 Laplace 分布和最大后验估计解释


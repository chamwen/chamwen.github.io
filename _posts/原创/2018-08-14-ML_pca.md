---
layout: post
title: 关于PCA的五问
date: 2018-08-14 15:22
tags: [PCA]
categories: Machine-Learning
description: "PCA 本质是特征选择，而且是无监督的特征选择，依据方差最大的方向（或者说子空间）是原始数据中的主要信号方向，但是因为是无监督的，和标签相关度不一定很高"
mathjax: true
---

* content
{:toc}
PCA 本质是特征选择，而且是无监督的特征选择，依据方差最大的方向（或者说子空间）是原始数据中的主要信号方向，但是因为是无监督的，和标签相关度不一定很高。先记一下 PCA 的几个疑问。 <!--more-->

**Cham's Blog 首发原创**



### 问题1：数据在PCA之前和之后需不需要进行标准化操作？

答案是必须的，使用之前进行 zscore 正态化处理，使用之后要进行中心化操作

```matlab
function fdata=PCANorm(data,numPCs)
[~,scores]=pca(zscore(data));
fdata=scores(:,1:numPCs);
fdata=(fdata-repmat(min(fdata,[],1),size(fdata,1),1))...
    *spdiags(1./(max(fdata,[],1)-min(fdata,[],1))',0,size(fdata,2),size(fdata,2));
end
```



### 问题2：在线系统怎么实现PCA更高效？

保留测试集的系数矩阵，直接把新的测试集样本和老的测试集样本组合在一起，用矩阵的乘法对测试集数据进行降维。离线的就是直接把训练集和测试集所有的数据都拿出来，直接降维，每来一个新的数据，这一过程都要重复一次。



### 问题3：如果只有一个样本，样本的维度是300，怎么降维？

这时用 PCA 去降维的话，维度只是 1，离线学习中使用留一法则不能使用 PCA。行向量的特征值只有一个，因为其 rank 为 1，所以在分解时，其特征值只有一个，其他的特征值接近于 0 或等于 0，没有意义。同样如果数据 $A\sim(m\times n)$，其中 $m<n$ 那么数据的特征值最多只有 $m$ 个可用。所以 rank(A) 是$A$ 降维后的最高维度。



### 问题4：对于文字数据，怎么使用PCA

```python
# 在转化成稀疏矩阵之后，用 SVD
from sklearn.decomposition import TruncatedSVD
```



### 问题5：Sklearn和MATLAB中PCA的参数

```python
X1 = PCA(n_components=20).fit_transform(X) # 选择指定数量的主成分
X2 = PCA(n_components=0.95).fit_transform(X) # 选择指定比例的主成分
X3 = PCA(n_components='mle').fit_transform(X) # 适用于样本数多于特征数，且svd_solver为‘full'时
```
matlab中PCA函数说明:
```matlab
[COEFF, SCORE, LATENT] = PCA(X)
[~, SCORE] = PCA(X) % 获得降维之后的数据
COEFF = PCA(X) % 获得特征向量
```
COEFF 是 X 对应的协方差阵 V 的特征向量矩阵，即变换矩阵或投影矩阵，其每列对应一个特征值的特征向量，列的排列顺序是按特征值的大小递减排序；SCORE是由原数据 X 转变到主成分空间所得到的数据；LATENT 是特征值，选择指定数量或者比例的主成分是按照 LATENT 来计算的。它们之间的变换关系是：
$$
SCORE=X\times COEFF=LATENT\times COEFF
$$
如果 $X\sim m\times n$，若 $n\ge m$，则 $COEFF$ 的维度是 $m-1$，若 $m>n$，则 $COEFF$ 的维度是 $n$，$COEFF$ 是原始数据的变换矩阵。
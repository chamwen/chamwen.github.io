---
layout: post
title: 深度学习tricks
date: 2019-07-24 8:35
tags: [droupout,BN,CART]
categories: Deep-Learning
description: "深度学习常用的技巧，trick"
mathjax: true
---

* content
{:toc}
报告中常听到的tricks做个了断。 <!--more-->

### Dropout

$r$ 为概率向量，为 0 或者 1

$$
\begin{align} r_{j}^{(l)} & \sim \text { Bernoulli }(p) \\ \widetilde{\mathbf{y}}^{(l)} &=\mathbf{r}^{(l)} * \mathbf{y}^{(l)} \\ z_{i}^{(l+1)} &=\mathbf{w}_{i}^{(l+1)} \widetilde{\mathbf{y}}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &=f\left(z_{i}^{(l+1)}\right) \end{align}
$$

### Batch Normalization

$m$ 指的是mini-batch的size

$$
\begin{align} \mu_{\mathcal{B}} & \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\ \sigma_{\mathcal{B}}^{2} & \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} \\ \widehat{x}_{i} & \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}} \\ y_{i} & \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \mathrm{BN}_{\gamma, \beta}\left(x_{i}\right) \end{align}
$$

### CART剪枝

剪枝前的损失函数，$\|T\|$ 表示模型复杂度，$\alpha$ 逐渐增大，剪枝过程中$\|T\|$ 不断变小，当 $\alpha$ 足够大时，模型停止剪枝

$$
C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha|T|
$$

$g(t)$ 表示剪枝后整体损失函数减少的程度，是对决策树各个节点是否需要被剪枝的一种度量。

$$
g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
$$

对于同一棵树的结点，$\alpha$ 都是一样的，当 $\alpha$ 从 0 开始缓慢增大，总会有某棵子树该剪，其他子树不该剪的情况，即alpha超过了某个结点的 $g(t)$，但还没有超过其他结点的 $g(t)$。这样随着 $\alpha$ 不断增大，不断地剪枝，就得到了 $n+1$ 棵子树，接下来只要用独立数据集测试这 $n+1$棵子树，选出误差最小的子树。对于某个 $\alpha$，剪枝时 $g(t)$ 选择最小的，整体误差小。

### Mini batch

梯度下降中每次更新权重 $w$ 和偏置项 $b$ 的数据，是为了在较小精度损失下减少内存和提高训练速度。

mini batch: 批，full batch的子集

iterations: 迭代，batch*iterations=full batch，每训练一个mini batch完成一次iteration

epoch: 遍历完一次full batch是一次epoch

### SGD & Adam

SGD步骤：

1. 计算目标函数关于当前参数的梯度

$$
g_{t}=\nabla f\left(w_{t}\right)
$$

2. 计算当前时刻的下降梯度 

$$
\eta_{t}=\alpha \cdot g_{t}
$$

3. 根据下降梯度进行更新

$$
w_{t+1}=w_{t}-\eta_{t}
$$

Adam步骤：

1. 计算目标函数关于当前参数的梯度

$$
g_{t}=\nabla f\left(w_{t}\right)
$$

2. 计算SGD的一阶动量和RMSProp的二阶动量，$\beta_1$和 $\beta_2$ 为超参数

$$
\begin{align} m_{t} &=\beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t} \\ V_{t} &=\beta_{2} \cdot V_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \end{align}
$$

3. 计算下降梯度，并更新

$$
w_{t+1}=w_{t}-\alpha \cdot \frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}}+\epsilon}
$$
